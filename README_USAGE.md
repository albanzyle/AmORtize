# Knapsack Neural Network - Usage Guide

## üìÅ Project Structure

```
AmORtize/
‚îú‚îÄ‚îÄ knapsack_nn.py              # Main training script
‚îú‚îÄ‚îÄ evaluate_knapsack.py        # Separate evaluation/benchmarking script
‚îú‚îÄ‚îÄ knapsack_model.pt           # Saved trained model (generated after training)
‚îú‚îÄ‚îÄ knapsack_config.json        # Training configuration (generated after training)
‚îú‚îÄ‚îÄ knapsack_results.json       # Training results (generated after training)
‚îî‚îÄ‚îÄ benchmark_results.json      # Benchmark results (generated after evaluation)
```

## üöÄ Quick Start

### 1. Train the Model

```bash
python knapsack_nn.py
```

This will:
- Train for 100 epochs on N=20 item problems
- Save the model to `knapsack_model.pt`
- Save config to `knapsack_config.json`
- Evaluate on multiple problem sizes (10, 20, 30, 50)
- Generate results in `knapsack_results.json`

### 2. Benchmark the Trained Model (Separate Script)

```bash
# Basic usage (small + medium problems only, no MILP)
python evaluate_knapsack.py

# With MILP solver for large problems (requires PuLP)
pip install pulp
python evaluate_knapsack.py --n-large 100 200 500

# Custom benchmark
python evaluate_knapsack.py \
    --model knapsack_model.pt \
    --n-small 10 20 30 50 \
    --n-large 100 200 \
    --instances-small 100 \
    --instances-large 30 \
    --milp-timeout 10.0 \
    --output my_benchmark.json
```

## üìä What Gets Benchmarked

### Small/Medium Problems (N=10-50)
- **Baseline**: Optimal DP solution
- **Metrics**: 
  - Quality gap (%)
  - Time comparison (NN vs DP)
  - Speedup factor
  - Feasibility rate

### Large Problems (N=100-500)
- **Baseline**: Time-limited MILP solver (default: 5 seconds)
- **Metrics**:
  - Quality gap vs MILP (%)
  - Time comparison (NN vs MILP)
  - Speedup factor
  - Feasibility rate

## üìà Expected Results

### Small Problems (N=20)
- **Gap**: ~0.5-1.5% from optimal
- **Speed**: 50-100x faster than DP
- **Feasibility**: 100%

### Large Problems (N=100-200)
- **Gap**: ~2-5% from 5-second MILP
- **Speed**: 100-1000x faster than MILP
- **Feasibility**: ~98-100%

## üîß Advanced Usage

### Load Model in Another Script

```python
import torch
from knapsack_nn import KnapsackNet, greedy_decode, compute_value

# Load model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = KnapsackNet(hidden_dim=128).to(device)
model.load_state_dict(torch.load('knapsack_model.pt', map_location=device))
model.eval()

# Solve a new instance
values = torch.tensor([[10, 20, 30, 40, 50]], dtype=torch.float32).to(device)
weights = torch.tensor([[5, 10, 15, 20, 25]], dtype=torch.float32).to(device)
capacity = torch.tensor([[30]], dtype=torch.float32).to(device)

with torch.no_grad():
    logits = model(values, weights, capacity)
    probs = torch.sigmoid(logits)
    solution = greedy_decode(probs, weights, capacity, 
                            use_local_search=True,
                            values_for_local_search=values)
    total_value = compute_value(solution, values)
    
print(f"Solution: {solution}")
print(f"Total Value: {total_value.item()}")
```

### Export to TorchScript (for Production)

```python
import torch
from knapsack_nn import KnapsackNet

model = KnapsackNet(hidden_dim=128)
model.load_state_dict(torch.load('knapsack_model.pt'))
model.eval()

# Export
scripted_model = torch.jit.script(model)
scripted_model.save('knapsack_model_scripted.pt')

# Load later (no Python dependencies needed!)
loaded = torch.jit.load('knapsack_model_scripted.pt')
```

## üéØ Key Features Implemented

‚úÖ **Model Saving/Loading** - Reuse trained models  
‚úÖ **2-Swap Local Search** - Improves solutions by ~0.5%  
‚úÖ **Deterministic Training** - Reproducible results  
‚úÖ **Multi-size Testing** - Generalization checks  
‚úÖ **Config/Results Export** - Track experiments  
‚úÖ **Separate Evaluation Script** - Benchmark independently  
‚úÖ **MILP Comparison** - Industry-standard baseline  
‚úÖ **Timing Benchmarks** - Speed vs quality tradeoffs  

## üìù Configuration Options

Edit `knapsack_nn.py` main function to customize:

```python
N_ITEMS = 20          # Training problem size
HIDDEN_DIM = 128      # Model capacity
N_EPOCHS = 100        # Training duration
BATCH_SIZE = 64       # Memory vs speed tradeoff
LEARNING_RATE = 0.001 # Convergence speed
REGRET_WEIGHT = 1.0   # Solution quality emphasis
USE_POS_WEIGHT = False # Class imbalance handling
```

## üêõ Troubleshooting

**PuLP not installed?**
```bash
pip install pulp
```
*Note: Evaluation script works without PuLP for small problems, just uses greedy baseline for large ones.*

**CUDA out of memory?**
- Reduce `BATCH_SIZE` in training
- Use CPU: `device = torch.device('cpu')`

**Model not generalizing?**
- Train on mixed sizes: modify `generate_knapsack_instance` to randomize N
- Increase `HIDDEN_DIM` to 256
- Train longer: increase `N_EPOCHS`

## üìö Files Generated

| File | Purpose | When Created |
|------|---------|--------------|
| `knapsack_model.pt` | Trained model weights | After training |
| `knapsack_config.json` | Hyperparameters used | After training |
| `knapsack_results.json` | Training evaluation | After training |
| `benchmark_results.json` | Full benchmark data | After evaluation |

## üöÄ Next Steps

1. **Experiment with hyperparameters** - Try different hidden dimensions or loss weights
2. **Test on real data** - Replace `generate_knapsack_instance` with your data
3. **Deploy to production** - Use TorchScript export for C++/mobile
4. **Scale up** - Train on larger problems (N=50-100)
5. **Try different architectures** - Add attention, GNN layers, etc.

---

**Need help?** Check the inline documentation in `knapsack_nn.py` and `evaluate_knapsack.py`
